##one-way anova: identity + PWB
boxplot(data$wellbeing~data$arsma1)
install.packages("car")
library(car)
leveneTest(data$wellbeing~data$arsma1)
myaov <- aov(data$wellbeing~data$arsma1)
Anova(myaov, type=5)
Anova(myaov, type=3)
TukeyHSD(myaov)
View(data)
View(data)
View(data)
library(xlsx)
install.packages("xlsx")
library(xlsx)
write.xlsx(mydata, "c:/mydata.xlsx")
write.xlsx(data, "c:/mydata.xlsx")
##one-way anova: identity + PWB
as.character(data$arsma1)
##one-way anova: identity + PWB
data$arsma1 <- as.character(data$arsma1)
boxplot(data$wellbeing~data$arsma1)
leveneTest(data$wellbeing~data$arsma1)
myaov <- aov(data$wellbeing~data$arsma1)
Anova(myaov, type=3)
TukeyHSD(myaov)
summary.lm(myaov)$r.squared
##one-way anova: identity + GPA
boxplot(data$cumGPA~data$arsma1)
leveneTest(data$cumGPA~data$arsma1)
myaov2 <- aov(data$cumGPA~data$arsma1)
Anova(myaov2, type=3)
TukeyHSD(myaov2)
TukeyHSD(myaov)
boxplot(data$blp_tot~data$arsma1)
ggplot(data, aes(x=arsma1, y=blp_tot)) + geom_boxplot()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot() +
theme.minimal()
boxplot(data$blp_tot~data$arsma1)
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot() +
theme_minimal()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot() +
theme_classic()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot() +
labs(x="Bicultural Level", y = "Bilingualism Score") +
theme_classic()
ggplot(data, aes(x=arsma1, y=blp_tot, fill=arsma1)) +
geom_boxplot() +
labs(x="Bicultural Level", y = "Bilingualism Score") +
theme_classic()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot(fill="gray") +
labs(x="Bicultural Level", y = "Bilingualism Score") +
theme_classic()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot(fill="gray") +
labs(x="Bicultural Level", y = "Bilingualism Score") +
scale_fill_grey() +
theme_classic()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot() +
scale_fill_grey() +
labs(x="Bicultural Level", y = "Bilingualism Score") +
theme_classic()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot() +
labs(x="Bicultural Level", y = "Bilingualism Score") +
scale_fill_grey() +
theme_classic()
ggplot(data, aes(x=arsma1, y=blp_tot)) +
geom_boxplot(fill="gray") +
labs(x="Bicultural Level", y = "Bilingualism Score") +
theme_classic()
ggplot(data, aes(x=arsma1, y=c(blp_spn, blp_eng))) + geom_bar()
data %>% group_by(arsma1) %>% summarize(English = mean(blp_eng), Spanish = mean(blp_spn))
bilingual <- data %>% group_by(arsma1) %>% summarize(English = mean(blp_eng), Spanish = mean(blp_spn))
View(bilingual)
bilingual <- pivot_longer(cols = c(English, Spanish),
names_to = Language, values_to = Score)
bilingual <- pivot_longer(cols = c(`English`, `Spanish`),
names_to = Language, values_to = Score)
bilingual <- pivot_longer(cols = !arsma1,
names_to = Language, values_to = Score)
bilingual <- pivot_longer(data = bilingual, cols = !arsma1,
names_to = Language, values_to = Score)
View(bilingual)
bilingual <- pivot_longer(data = bilingual, cols = !arsma1,
names_to = "Language", values_to = "Score")
ggplot(bilingual, aes(x=arsma1, y=Score, fill=Language)) +
geom_bar(stat='identity', position='dodge')
ggplot(bilingual, aes(x=arsma1, y=Score, fill=Language)) +
geom_bar(stat='identity', position='dodge') +
theme_minimal()
ggplot(bilingual, aes(x=arsma1, y=Score, fill=Language)) +
geom_bar(stat='identity', position='dodge') +
theme_classic()
ggplot(bilingual, aes(x=arsma1, y=Score, fill=Language)) +
geom_bar(stat='identity', position='dodge') +
theme(legend.position="none") +
theme_classic()
ggplot(bilingual, aes(x=arsma1, y=Score, fill=Language)) +
geom_bar(stat='identity', position='dodge') +
theme(legend.position="bottom") +
theme_classic()
ggplot(bilingual, aes(x=arsma1, y=Score, fill=Language)) +
scale_fill_grey() +
geom_bar(stat='identity', position='dodge') +
theme(legend.position="bottom") +
theme_classic()
ggplot(bilingual, aes(x=arsma1, y=Score, fill=Language)) +
scale_fill_grey() +
geom_bar(stat='identity', position='dodge') +
labs(x="Bicultural Level", y = "Language Score") +
theme(legend.position="bottom") +
theme_classic()
View(data)
as.numeric(data$arsma1)
data$arsma1 <- as.numeric(data$arsma1)
data$inter <- data$arsma1*data$wellbeing
summary(lm(wellbeing~arsma1+blp_tot+inter, data=data))
ggplot(data, aes(x=arsma1, y=wellbeing)) +
labs(x="Bicultural Level", y = "PWB Score") +
abline(98.92, 1)
ggplot(data, aes(x=arsma1, y=wellbeing)) +
labs(x="Bicultural Level", y = "PWB Score") +
abline(27.97, -.09, col='blue')
plot(data$arsma1, data$wellbeing, type='n',
+      xlab='Stress', ylab='Depression')
plot(data$arsma1, data$wellbeing, type='n', xlab='Stress', ylab='Depression')
plot(data$arsma1, data$wellbeing, type='n',
xlab='Bicultural Level', ylab='PWB Score')
plot(data$arsma1, data$wellbeing, type='n',
xlab='Bicultural Level', ylab='PWB Score')
abline(28.65, 1)
abline(27.97, -.09, col='blue')
abline(27.30, -1.19, col='red')
data$inter <- data$arsma1*data$cumGPA
summary(lm(cumGPA~arsma1+blp_tot+inter, data=data))
reticulate::repl_python()
import numpy as np
np.random.seed( 322 )
class_diag <- function(score, truth, positive, cutoff=.5){
pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
tab<-table(truth, pred)
acc=sum(diag(tab))/sum(tab)
sens=tab[1,1]/rowSums(tab)[1]
spec=tab[2,2]/rowSums(tab)[2]
ppv=tab[1,1]/colSums(tab)[1]
#CALCULATE F1
f1=2*(sens*ppv)/(sens+ppv)
#CALCULATE EXACT AUC
truth<-as.numeric(truth=="TRUE")
ord<-order(score, decreasing=TRUE)
score <- score[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(score[-1]>=score[-length(score)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
data <- read_csv("/Documents/project2/FirstYearGPA.csv")
data <- read.csv("/Documents/project2/FirstYearGPA.csv")
data <- read.csv("Documents/project2/FirstYearGPA.csv")
data <- read.csv("Documents/project2/FirstYearGPA.csv")
data <- read.csv("Documents/FirstYearGPA.csv")
data <- read.csv("Documents/project2/FirstYearGPA.csv")
data <- read.csv("Documents/project2/FirstYearGPA.csv")
data <- read.csv("/Documents/project2/FirstYearGPA.csv")
data <- read.csv("~/Documents/project2/FirstYearGPA.csv")
View(data)
data <- read.csv("~/Documents/project2/MedGPA.csv")
View(data)
library(tidyverse)
library(cluster)
sil_width <- vector() #empty vector to hold mean sil width
for(i in 2:10){
kms <- kmeans(data, centers=i) #compute k-means solution for each k
sil <- silhouette(kms$cluster,dist(data)) #get sil widths
sil_width[i]<-mean(sil[,3]) #take averages (higher is better)
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) +
scale_x_continuous(name = "k", breaks=1:10)
sil_width <- vector() #empty vector to hold mean sil width
for(i in 2:10){
kms <- kmeans(data, centers=i) #compute k-means solution for each k
sil <- silhouette(kms$cluster,dist(data)) #get sil widths
sil_width[i]<-mean(sil[,3]) #take averages (higher is better)
}
sil_width <- vector() #empty vector to hold mean sil width
for(i in 2:10){
kms <- kmeans(data, centers = i) #compute k-means solution for each k
sil <- silhouette(kms$cluster, dist(data)) #get sil widths
sil_width[i] <- mean(sil[,3]) #take averages (higher is better)
}
kms <- kmeans(data, centers = i)
library(cluster)
sil_width <- vector() #empty vector to hold mean sil width
for(i in 2:10){
kms <- kmeans(data, centers = i) #compute k-means solution for each k
sil <- silhouette(kms$cluster, dist(data)) #get sil widths
sil_width[i] <- mean(sil[,3]) #take averages (higher is better)
}
data_omit <- na.omit(data)
sil_width <- vector() #empty vector to hold mean sil width
for(i in 2:10){
kms <- kmeans(data_omit, centers = i, ) #compute k-means solution for each k
sil <- silhouette(kms$cluster, dist(data_omit)) #get sil widths
sil_width[i] <- mean(sil[,3]) #take averages (higher is better)
}
kms <- kmeans(data_omit, centers = i)
data[apply(sapply(data, is.finite), 1, all),]
kms <- kmeans(data_omit, centers = i)
clust_dat <- data %>% select(BCPM, GPA, VR, PS, WS, BS, MCAT, Apps)
View(clust_dat)
kms <- kmeans(clust_dat, centers = i)
clust_dat <- data_omit %>% select(BCPM, GPA, VR, PS, WS, BS, MCAT, Apps)
kms <- kmeans(clust_dat, centers = i)
sil_width <- vector() #empty vector to hold mean sil width
for(i in 2:10){
kms <- kmeans(clust_dat, centers = i) #compute k-means solution for each k
sil <- silhouette(kms$cluster, dist(clust_dat)) #get sil widths
sil_width[i] <- mean(sil[,3]) #take averages (higher is better)
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) +
scale_x_continuous(name = "k", breaks=1:10)
##pam
pam1 <- clust_dat %>% pam(k=3)
data %>% slice(pam1$id.med)
##adjusting data for kmeans()
data_omit <- na.omit(data)
clust_dat <- data_omit %>% select(BCPM, GPA, VR, PS, WS, BS, MCAT, Apps)
sil_width <- vector()
for(i in 2:10){
kms <- clust_dat %>% scale %>% kmeans(3) ##kmeans(clust_dat, centers = i)
sil <- silhouette(kms$cluster, dist(clust_dat))
sil_width[i] <- mean(sil[,3])
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) +
scale_x_continuous(name = "k", breaks=1:10)
clust_dat <- data_omit %>% select(BCPM, GPA, VR, PS, WS, BS, MCAT, Apps)
sil_width <- vector()
for(i in 2:10){
kms <- clust_dat %>% scale %>% kmeans(centers = i)
##kmeans(clust_dat, centers = i)
sil <- silhouette(kms$cluster, dist(clust_dat))
sil_width[i] <- mean(sil[,3])
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) +
scale_x_continuous(name = "k", breaks=1:10)
pam1 <- clust_dat %>% pam(k=2)
data %>% slice(pam1$id.med)
library(GGally)
library(GGally)
Pottery %>% mutate(cluster=as.factor(pam1$clustering)) %>%
ggpairs(columns = c("BCPM","GPA","VR","PS","WS","BS","MCAT","Apps"), aes(color=cluster))
library(GGally)
data_omit %>% mutate(cluster=as.factor(pam1$clustering)) %>%
ggpairs(columns = c("BCPM","GPA","VR","PS","WS","BS","MCAT","Apps"), aes(color=cluster))
pamclust <- clust_dat %>% mutate(cluster=as.factor(pam1$clustering))
pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)
data %>% slice(pam1$id.med)
##goodness of fit
plot(pam1, which=2)
pam1 <- clust_dat %>% pam(k=2)
library(GGally)
data_omit %>% mutate(cluster=as.factor(pam1$clustering)) %>%
ggpairs(columns = c("BCPM","GPA","VR","PS","WS","BS","MCAT","Apps"), aes(color=cluster))
pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)
pam1 <- clust_dat %>% pam(k=2)
library(GGally)
data_omit %>% mutate(cluster=as.factor(pam1$clustering)) %>%
ggpairs(columns = c("BCPM","GPA","VR","PS","WS","BS","MCAT","Apps"), aes(color=cluster))
##goodness of fit
plot(pam1, which=2)
pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)
##final medoids
data %>% slice(pam1$id.med)
pamclust <- clust_dat %>% mutate(cluster=as.factor(pam1$clustering))
pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)
plot(pam1, which=1)
View(data_omit)
View(data_omit)
##eigen
eig1 <- data_omit %>% select(BCPM, GPA, MCAT, Apps) %>% cor() %>% eigen()
eig1$values
sum(eig1$values)
eig1$values
##eigen
eig1 <- data_omit %>% select(BCPM, GPA, VR, PS, WS, BS, MCAT, Apps) %>%
cor() %>% eigen()
eig1$values
eig1$vectors
sum(eig1$values)
eig1$values
eig1$vectors
X <- data_omit %>% select(5:12) %>% scale
head(round(X,3))
plot(pam1,which=1)
PCAscores <- X %*% eig1$vectors
data_omit %>% mutate(PC1=PCAscores[,1], PC2=PCAscores[,2]) %>%
ggplot(aes(PC1,PC2,color=Accept)) + geom_point()
eig1$values
eig1$vectors
data_omit %>% mutate(PC1=PCAscores[,1], PC2=PCAscores[,2]) %>%
ggplot(aes(PC1,PC2,color=Accept)) + geom_point()
eig1$vectors
eig1 <- data_omit %>% select(BCPM, GPA, VR, PS, WS, BS, MCAT, Apps) %>%
cor() %>% eigen()
eig1$values
eig1$vectors
sum(eig1$values)
X <- data_omit %>% select(5:12) %>% scale
PCAscores <- X %*% eig1$vectors
data_omit %>% mutate(PC1=PCAscores[,1], PC2=PCAscores[,2]) %>%
ggplot(aes(PC1,PC2,color=Accept)) + geom_point()
##other approach
dataPCA <- data_omit %>% select(5:12, Accept)
View(dataPCA)
data_nums <- dataPCA %>% select_if(is.numeric) %>% scale
data2 <- data_omit %>% select(5:12, Accept)
data_nums <- data2 %>% select_if(is.numeric) %>% scale
dataPCA <- princomp(data_nums)
names(dataPCA)
summary(dataPCA, loadings=T)
eigen(cor(data_nums))
datadf <- data.frame(Name = data2$Name,
PC1 = dataPCA$scores[,1],
PC2 = dataPCA$scores[,2])
datadf <- data.frame(PC1 = dataPCA$scores[,1],
PC2 = dataPCA$scores[,2])
ggplot(datadf, aes(PC1, PC2)) + geom_point()
datadf <- data.frame(Name = data_omit$X,
PC1 = dataPCA$scores[,1],
PC2 = dataPCA$scores[,2])
ggplot(datadf, aes(PC1, PC2)) + geom_point()
dataPCA$scores[,1:4] %>% as.data.frame  %>% top_n(3, Comp.1)
dataPCA$scores[,1:4] %>% as.data.frame  %>% top_n(-3, Comp.1)
dataPCA$scores[,1:4] %>% as.data.frame  %>% top_n(3, Comp.1)
ggplot(datadf, aes(PC1, PC2)) + geom_point()
ggplot(datadf, aes(PC1, PC2), fill=data_omit$Accept) + geom_point()
ggplot(datadf, aes(PC1, PC2, fill=data_omit$Accept)) + geom_point()
ggplot(datadf, aes(PC1, PC2)) + geom_point(fill=data_omit$Accept)
ggplot(datadf, aes(PC1, PC2, fill=data_omit$Accept)) + geom_point()
ggplot(datadf, aes(PC1, PC2, color=data_omit$Accept)) + geom_point()
data2 <- data_omit %>% select(5:12, Accept)
data_nums <- data2 %>% select_if(is.numeric) %>% scale
dataPCA <- princomp(data_nums)
summary(dataPCA, loadings=T)
datadf <- data.frame(Name = data_omit$X,
PC1 = dataPCA$scores[,1],
PC2 = dataPCA$scores[,2])
ggplot(datadf, aes(PC1, PC2, color=data_omit$Accept)) + geom_point()
summary(dataPCA, loadings=T)
##PCA plot
ggplot(datadf, aes(PC1, PC2, color=data_omit$Accept)) + geom_point()
summary(dataPCA, loadings=T)
##PCA plot
ggplot(datadf, aes(PC1, PC2, color=data_omit$Accept)) + geom_point()
# linear classifier code here
data_omit %>% ggplot(aes(MCAT, Acceptance)) +
geom_point() +
geom_smooth(method="lm", se=F) +
ylim(0,1)
fit <- lm(Acceptance ~ MCAT, data=data_omit)
score <- predict(fit)
score %>% round(3)
class_diag(score,truth=data_omit$Acceptance, positive=1)
fit <- glm(Acceptance ~ MCAT, data=data_omit, family="binomial")
score <- predict(fit, type="response")
score %>% round(3)
class_diag(score, truth=data_omit$Acceptance, positive=1)
fit <- glm(Acceptance ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=data_omit,
family="binomial")
score <- predict(fit, type="response")
class_diag(score, data_omit$Acceptance, positive=1)
table(truth = data_omit$Acceptance, predictions = score > .5)
##cross-validation of linear classifier
library(caret)
##cross-validation of linear classifier
set.seed(1234)
k=10 #choose number of folds
data3 <- data_omit[sample(nrow(data_omit)),] #randomly order rows
folds <- cut(seq(1:nrow(data_omit)),breaks=k,labels=F) #create folds
diags <- NULL
for(i in 1:k){
## Create training and test sets
train <- data3[folds!=i,]
test <- data3[folds==i,]
truth <- test$y ## Truth labels for fold i
## Train model on training set (all but fold i)
fit <- glm(y~clump_thickness,data=train,family="binomial")
## Test model on test set (fold i)
probs <- predict(fit,newdata = test,type="response")
## Get diagnostics for fold i
diags <- rbind(diags,class_diag(probs,truth, positive=1))
}
View(test)
set.seed(1234)
k=10 #choose number of folds
data3 <- data_omit[sample(nrow(data_omit)),] #randomly order rows
folds <- cut(seq(1:nrow(data_omit)),breaks=k,labels=F) #create folds
diags <- NULL
for(i in 1:k){
## Create training and test sets
train <- data3[folds!=i,]
test <- data3[folds==i,]
truth <- test$Acceptance ## Truth labels for fold i
## Train model on training set (all but fold i)
fit <- glm(Acceptance ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps, data=train, family="binomial")
## Test model on test set (fold i)
probs <- predict(fit,newdata = test,type="response")
## Get diagnostics for fold i
diags <- rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags, mean)
class_diag(score, data_omit$Acceptance, positive=1)
library(caret)
knn_fit <- knn3(Acceptance ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=data_omit)
prob_knn <- predict(knn_fit, data_omit)
class_diag(prob_knn[,2], data_omit$Acceptance, positive="True")
library(caret)
knn_fit <- knn3(Acceptance==1 ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=data_omit)
prob_knn <- predict(knn_fit, data_omit)
class_diag(prob_knn[,2], data_omit$Acceptance, positive="True")
View(prob_knn)
library(caret)
knn_fit <- knn3(Acceptance==1 ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=data_omit)
prob_knn <- predict(knn_fit, data_omit)[,2]
class_diag(prob_knn, data_omit$Acceptance, positive="True")
library(caret)
knn_fit <- knn3(Acceptance==1 ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=data_omit)
prob_knn <- predict(knn_fit, data_omit)[,2]
class_diag(prob_knn, data_omit$Acceptance, positive="True")
table(truth = data_omit$Acceptance, predictions = probs>.5)
library(caret)
knn_fit <- knn3(Acceptance==1 ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=data_omit)
prob_knn <- predict(knn_fit, newdata=data_omit)[,2]
class_diag(prob_knn, data_omit$Acceptance, positive="True")
table(truth = data_omit$Acceptance, predictions = score > .5)
table(truth = data_omit$Acceptance, predictions = prob_knn > .5)
library(caret)
knn_fit <- knn3(Acceptance==1 ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=data_omit)
prob_knn <- predict(knn_fit, newdata=data_omit)[,2]
class_diag(prob_knn, data_omit$Acceptance, positive=1)
set.seed(322)
k=10
data4 <- sample_frac(data_omit)
folds <- rep(1:k, length.out=nrow(data4))
diags <- NULL
i=1
for(i in 1:k){
# create training and test sets
train <- data4[folds!=i,]
test <- data4[folds==i,]
truth <- test$Acceptance
# train model
fit <- knn3(Acceptance==1 ~ BCPM + GPA + VR + PS + WS + BS + MCAT + Apps,
data=train)
### SPECIFY THE LOGISTIC REGRESSION MODEL FIT TO THE TRAINING SET HERE
# test model
probs <- predict(fit, newdata = test)[,2]
### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE
# get performance metrics for each fold
diags <- rbind(diags, class_diag(probs,truth,positive = 1)) }
#average performance metrics across all folds
summarize_all(diags,mean)
##regression model code
fit <- lm(Apps ~ GPA + MCAT, data=data_omit)
yhat <- predict(fit)
##mse
mean((data_omit$Apps-yhat)^2)
set.seed(1234)
k=10
data5 <- data_omit[sample(nrow(data_omit)),]
folds <- cut(seq(1:nrow(data_omit)),breaks=k,labels=F)
diags <- NULL
for(i in 1:k){
train <- data5[folds!=i,]
test <- data5[folds==i,]
fit <- lm(Apps ~ GPA + MCAT, data=train)
yhat <- predict(fit, newdata=test)
diags <- mean((test$Apps-yhat)^2)
}
##average mse
mean(diags)
library(reticulate)
corGPA <- data_omit %>% select(GPA, BCPM)
View(corGPA)
subMCAT <- data_omit %>% select(VR, PS, WS, BS)
reticulate::repl_python()
py$subMCAT$totMCAT
View(subMCAT)
py$subMCAT
reticulate::repl_python()
py$totMCAT
reticulate::repl_python()
py$totMCAT
reticulate::repl_python()
